{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c4b7ec38-acaa-40a8-bc8f-00020030e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas requests tqdm kaggle beautifulsoup4 lxml scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "84d6984d-0f4b-48ed-a118-f0b98415aadd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please select a dataset:\n",
      " 1:Enron\n",
      " 2:Figshare\n",
      " 3:NaserPhishingDataset\n",
      " 4:Cyber Cop\n",
      " 2\n",
      "Please select A Minor Dataset:\n",
      " 1:Assassin\n",
      " 2:CEAS-08\n",
      " 3:Enron\n",
      " 4:Ling\n",
      " 5:TREC-05\n",
      " 6:TREC-06\n",
      " 7:TREC-07\n",
      " 2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'ntpath' has no attribute 'isFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 130\u001b[0m\n\u001b[0;32m    128\u001b[0m db_choice \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    129\u001b[0m which_dataset_minor,dataset_selection\u001b[38;5;241m=\u001b[39mselect()\n\u001b[1;32m--> 130\u001b[0m preprocess_dataset(which_dataset_minor,dataset_selection)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_dataset_basic\u001b[39m(text):\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    Clean and normalise Email text  BY:\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    1. Convert ot lowercase\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 207\u001b[0m, in \u001b[0;36mpreprocess_dataset\u001b[1;34m(dataset_selection, which_dataset_minor)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(data_path):\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Path does not exist:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misFile(data_path):\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath is a file, not a folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(data_path):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ntpath' has no attribute 'isFile'"
     ]
    }
   ],
   "source": [
    "#Phising Emails\n",
    "#Prepare raw emails by combinding subject +body\n",
    "#Format for \n",
    "#CSV FORMAT: sender, receiver, date, subject, body, label, urls\n",
    "#Basic goal:predict labels(0=ham, 1=spam)\n",
    "\n",
    "'''\n",
    "\n",
    "Preprocessing:\n",
    "Specfically for the \n",
    "CSV FORMAT: sender, receiver, date, subject, body, label, urls\n",
    "Basic goal:predict labels(0=ham, 1=spam)\n",
    "Actions:Combind subject and body, CONVERT ALL TO LOWERCASE!!! \n",
    "\n",
    "\n",
    "'''\n",
    "#! pip install nltk\n",
    "import nltk\n",
    "#nltk.download('all')DID Y\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words=set(stopwords.words('english'))\n",
    "extra_StopWords={\n",
    "       \"would\", \"could\", \"should\", \"will\", \"get\", \"know\", \"use\", \"say\", \"see\", \"make\",\n",
    "    \"go\", \"like\", \"also\", \"thank\", \"regards\", \"please\", \"dear\", \"let\", \"us\", \"ok\",\"\"\n",
    "}\n",
    "#ADD EXTRA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "#1:Enrol\n",
    "#2:https://figshare.com/articles/dataset/Seven_Phishing_Email_Datasets/25432108\n",
    "#3:https://www.kaggle.com/datasets/naserabdullahalam/phishing-email-dataset/data\n",
    "#4:https://www.kaggle.com/datasets/subhajournal/phishingemails?resource=download\n",
    "#:https://www.kaggle.com/datasets/venky73/spam-mails-dataset\n",
    "\n",
    "#need to compare and check theres no reapeats i think there are\n",
    "\n",
    "def select():\n",
    "   \n",
    "    dataset_selection=int(input(\"Please select a dataset:\\n 1:Enron\\n 2:Figshare\\n 3:NaserPhishingDataset\\n 4:Cyber Cop\\n\"))\n",
    "    match dataset_selection:\n",
    "        case 1:\n",
    "            data_folder = '../data/raw/enron_spam_data-master'\n",
    "            dataset='enron_spam_data.csv'\n",
    "            which_dataset_minor = 0\n",
    "           \n",
    "            dataset_path = os.path.join(data_folder, dataset)\n",
    "        case 2:\n",
    "            which_dataset_minor=int(input(\"Please select A Minor Dataset:\\n 1:Assassin\\n 2:CEAS-08\\n 3:Enron\\n 4:Ling\\n 5:TREC-05\\n 6:TREC-06\\n 7:TREC-07\\n\"))\n",
    "            #df=pd.read_csv('../data/raw/Seven_Phishing_Email_Datasets')\n",
    "            \n",
    "            match which_dataset_minor:\n",
    "                case 1:\n",
    "                    dataset='Assassin.csv'\n",
    "                    #columns:[sender, receiver, date, subject, body, label, urls, email_text, clean_email_text, url_count]\n",
    "                case 2:\n",
    "                    dataset='CEAS-08.csv'\n",
    "                    #columns:[sender, receiver, date, subject, body, label, urls, email_text, clean_email_text, url_count]\n",
    "            #    case 3:\n",
    "             #       dataset='Enron.csv'\n",
    "                   #columns:[sender, receiver, date, subject, body, label, urls, email_text, clean_email_text, url_count]\n",
    "                case 3:\n",
    "                    dataset='Ling.csv'\n",
    "                   #columns:[sender, receiver, date, subject, body, label, urls, email_text, clean_email_text, url_count]\n",
    "                case 4:\n",
    "                    dataset='TREC-05.csv'\n",
    "                case 5:\n",
    "                    dataset='TREC-06.csv'\n",
    "                case 6:\n",
    "                    dataset='TREC-07.csv'\n",
    "                case _:\n",
    "                    which_dataset_minor = 0\n",
    "                    raise ValueError(\"Unknown dataset number selected.\")\n",
    "                    \n",
    "            data_folder = '../data/raw/Seven_Phishing_Email_Datasets'\n",
    "            dataset_path = os.path.join(data_folder, dataset)\n",
    "            #need to add a case if not found?\n",
    "       #print(df.head(1))\n",
    "        case 3:\n",
    "          #Nasers\n",
    "            which_dataset_minor=int(input(\"Please select A Minor Dataset:\\n 1:SpamAssassin\\n 2:CEAS-08\\n 3:Enron\\n 4:Ling\\n 5:Nigerian_Fraud\\n 6:Nazario\\n 7:Labeled Datasets\\n\"))\n",
    "          \n",
    "            match which_dataset_minor:\n",
    "               # case 1:#This may be a duplicate Dataset that has been reused\n",
    "                #    dataset='SpamAssasin.csv'\n",
    "                    #Columns: Subject , Body, Label\n",
    "                case 1:\n",
    "                     dataset='CEAS-08.csv'\n",
    "              #  case 3:#This is a duplicate Dataset\n",
    "             #       dataset='Enron.csv'\n",
    "                case 2:\n",
    "                    dataset='Nigerian_Fraud.csv'\n",
    "                    #Columns: Sender,Reciever,Date,Subject,Body,Urls,Label\n",
    "                case 3:\n",
    "                    dataset='Nazario.csv'\n",
    "                case 4:\n",
    "                    dataset='phishing_email.csv'\n",
    "                    #Columns: text_combined,label\n",
    "                case _:\n",
    "                     which_dataset_minor = 0\n",
    "                     raise ValueError(\"Unknown dataset number selected.\")\n",
    "            data_folder = '../data/raw/Naser-Phishing-Dataset'\n",
    "            dataset_path = os.path.join(data_folder, dataset)\n",
    "        case 4:\n",
    "            #nneed to convert COLUMN 3 AS ITS CAT AS SAFE EMAIL OR PHISING EMAIKL NOT 0 OR 1\n",
    "            dataset_path = '../data/raw/Phishing_Email.csv'\n",
    "            which_dataset_minor=0\n",
    "        case _:\n",
    "            print(\"Invalid\")\n",
    "    #df=pd.read_csv('../data/raw/')\n",
    "    return which_dataset_minor, dataset_selection\n",
    "db_choice =1\n",
    "which_dataset_minor,dataset_selection=select()\n",
    "preprocess_dataset(which_dataset_minor,dataset_selection)\n",
    "\n",
    "def clean_dataset_basic(text):\n",
    "    '''\n",
    "    Clean and normalise Email text  BY:\n",
    "    1. Convert ot lowercase\n",
    "    2.replace mail/urls with blank\n",
    "    3.remove punctuation,digits and extra whitespace\n",
    "    4. remove stopwords\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "    text=text.lower()\n",
    "    #Potenially we could extarct with urls and sender stuff\n",
    "    #Easiest way would be to detect based on sender recieved and patterns but would eventually be invalid\n",
    "    text=re.sub(r\"\\S+@\\s+\",\"EMAIL\",text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|mailto:\\S+\", \"\", text)#URLS purger\n",
    "    #Remove punctuation,digits and special chars\n",
    "    #older\n",
    "    # text = re.sub(r\"[^a-z\\s]\", \" \", text)      #Remove any punct SPECIAL CHARS OR NUMS\n",
    "    text=re.sub(r\"\\s\", \" \",text).strip()#Remove extra spaces\n",
    "    #split and remove Stop words\n",
    "    words=text.split()\n",
    "    words= [x for x in words if x not in stop_words and len(x)>1]\n",
    "    #Rejoin \n",
    "    text=' '.join(words)\n",
    "  \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "\n",
    "#MAIN PREPROCESS\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset_selection, which_dataset_minor):\n",
    "    #datset mapped\n",
    "    match dataset_selection:\n",
    "            case 1:\n",
    "                     dataset='enron_spam_data.csv'\n",
    "                     data_folder = '../data/raw/enron_spam_data-master'\n",
    "                     which_dataset_minor = 0\n",
    "            case 2:\n",
    "                match which_dataset_minor:\n",
    "                    case 1:\n",
    "                         dataset='Assassin.csv'\n",
    "                    case 2:\n",
    "                         dataset='CEAS-08.csv'\n",
    "                    case 3:\n",
    "                         dataset='Ling.csv'\n",
    "                    case 4:\n",
    "                         dataset='TREC-05.csv'\n",
    "                    case 5:\n",
    "                          dataset='TREC-06.csv'\n",
    "                    case 6:\n",
    "                          dataset='TREC-07.csv'\n",
    "                    case _:\n",
    "                          raise ValueError(\"Unknown dataset number selected.\")\n",
    "                data_folder = '../data/raw/Seven_Phishing_Email_Datasets'\n",
    "            case 3:\n",
    "                match which_dataset_minor: \n",
    "                    case 1:\n",
    "                        dataset='CEAS-08.csv'\n",
    "                    case 2:\n",
    "                        dataset='Nigerian_Fraud.csv'\n",
    "                        #Columns: Sender,Reciever,Date,Subject,Body,Urls,Label\n",
    "                    case 3:\n",
    "                        dataset='Nazario.csv'\n",
    "                    case 4:\n",
    "                        dataset='phishing_email.csv'\n",
    "                        #Columns: text_combined,label\n",
    "                    case _:\n",
    "                         raise ValueError(\"Unknown dataset number selected.\")\n",
    "                data_folder = '../data/raw/Naser-Phishing-Dataset'\n",
    "            case 4:\n",
    "                #nneed to convert COLUMN 3 AS ITS CAT AS SAFE EMAIL OR PHISING EMAIKL NOT 0 OR 1\n",
    "                dataset_path = '../data/raw/Phishing_Email.csv'\n",
    "            \n",
    "            case _:\n",
    "                raise ValueError(\"Unknown dataset number selected.\")\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    data_path = os.path.join(data_folder, dataset)\n",
    "    #if not os.path.exists(data_path):\n",
    "     #   print(f\" Path does not exist:{data_path}\")\n",
    "   ## elif os.path.isfile(data_path):\n",
    "     #   print(f\"Path is a file, not a folder: {data_path}\")\n",
    "\n",
    "    \n",
    "   # elif os.path.isdir(data_path):\n",
    "   #     print(f\"Path is a folder with {len(os.listdir(data_path))} files\")\n",
    "   # else:\n",
    "    #    print(\"Error in loading path type!\")\n",
    "    \n",
    "    df=pd.read_csv(data_path,encoding='latin1',low_memory=False)\n",
    "    print(f\"Loaded {data_path}\\n with shape:{df.shape}\")\n",
    "   # df['text']=df['subject'].fillna(\"\")+\" \"+df['body'].fillna(\"\")\n",
    "#CHECK THAT THE AVBOVE IS OK WITH ALL\n",
    "#dataset_path.shape\n",
    "\n",
    "    #Building Text Up\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Data Cleaning\n",
    "    #Replace blank columns or all null columns with \"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().all() or(df[col].str.strip()=='').all():\n",
    "            print(f\"Column {col} has been detected as empty.\\n Replacing with empty string.\")\n",
    "            df[col]=\"\"\n",
    "            \n",
    "  #BUILD TEXT PROP\n",
    "    if 'subject' in df.columns and 'body' in df.columns:\n",
    "        df['email_text'] =df['subject'].fillna('')+ ' ' +df['body'].fillna('')\n",
    "    elif 'body' in df.columns:\n",
    "        df['email_test']=df['body'].fillna('')\n",
    "    else:\n",
    "        text_col = next((col for col in df.columns if df[col].dtype == object), None)\n",
    "        df['email_text'] = df[text_col].fillna('') if text_col else ''\n",
    "            \n",
    "        #Clean the email text\n",
    "        df['cleaned_text']=df['email_text'].apply(clean_dataset_basic)\n",
    "        \n",
    "        #If you want to force Columns we can use this\n",
    "        expected_cols = [\n",
    "            'sender', 'receiver', 'date', 'subject', 'body', 'label',\n",
    "            'urls', 'email_text', 'clean_email_text', 'url_count'\n",
    "        ]\n",
    "        for col in expected_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "    \n",
    "    df_processed = df[expected_cols]\n",
    "\n",
    "    \n",
    "    #Save files and outter\n",
    "    preprocessed_name=dataset.replace('.csv','_preprocessed.csv')\n",
    "    output_path=os.path.join(data_folder,preprocessed_name)\n",
    "    df_preprocessed.to_csv(data_folder,index=False,encoding='utf-8')\n",
    "    print(f\" Saved preprocessed file {dataset.name} to: {data_folder}\")\n",
    "    print(\"\\n🔍 Sample Check:\")\n",
    "    print(df_processed[['label', 'email_text', 'clean_email_text']].head(2))\n",
    "\n",
    "    example_text = \"hpl actuals for november teco tap hpl gas daily ls hpl lsk ic enron\"\n",
    "    print(\"\\n🧪 Example cleaning check:\")\n",
    "    print(\"Original:\", example_text)\n",
    "    print(\"Cleaned :\", clean_dataset_basic(example_text))\n",
    "    \n",
    "    #df=pd.read_csv('../data/raw/')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d86f9-33bc-4632-a68f-ca2ebd2543d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70d1ce-f4e5-4174-9db2-b1c5bf55cf99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
